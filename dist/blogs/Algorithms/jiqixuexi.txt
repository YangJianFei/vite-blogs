/*
title:机器学习分类算法之支持向量机
date:2022-01-11
keyword:机器学习,向量机
*/

<div id="content_views" class="htmledit_views">
                    <p id="main-toc"><span style="color:#4da8ee;"><strong>目录</strong></span></p> 
<p id="" style="margin-left:40px;"><a href="#t0" target="_self">支持向量机算法背景介绍</a></p> 
<p id="" style="margin-left:80px;"><a href="#t1" target="_self">什么是线性可分？</a></p> 
<p id="" style="margin-left:80px;"><a href="#t2" target="_self">什么又是超平面？</a></p> 
<p id="" style="margin-left:80px;"><a href="#t3" target="_self">支持向量机的三种情况</a></p> 
<p id="" style="margin-left:80px;"><a href="#t4" target="_self">近线性可分</a></p> 
<p id="" style="margin-left:80px;"><a href="#t5" target="_self">线性不可分</a></p> 
<p id="" style="margin-left:80px;"><a href="#t6" target="_self">不用核函数的传统方法</a></p> 
<p id="" style="margin-left:80px;"><a href="#t7" target="_self">核函数Kernel是什么？</a></p> 
<p id="" style="margin-left:80px;"><a href="#t8" target="_self">核函数SVM求解过程</a></p> 
<p id="" style="margin-left:80px;"><a href="#t9" target="_self">核函数的本质</a></p> 
<p id="" style="margin-left:40px;"><a href="#t10" target="_self">代码实例</a></p> 
<p id="" style="margin-left:80px;"><a href="#t11" target="_self">模型调参</a></p> 
<p id="" style="margin-left:80px;"><a href="#t12" target="_self">gamma调参</a></p> 
<p id="" style="margin-left:80px;"><a href="#t13" target="_self">C值调参</a></p> 
<p id="" style="margin-left:80px;"><a href="#t14" target="_self">使用Polynomial kernel进行预测</a></p> 
<p id="" style="margin-left:80px;"><a href="#t15" target="_self">使用RBF kernel进行预测</a></p> 
<p id="" style="margin-left:80px;"><a href="#t16" target="_self">总结</a></p> 
<p id="" style="margin-left:80px;"><a href="#t17" target="_self">每文一语</a></p> 
<hr id="hr-toc">
<h3><a name="t0"></a>&nbsp;👇👇🧐🧐✨✨🎉🎉</h3> 
<p><strong>欢迎点击专栏其他文章（欢迎订阅·持续更新中~）</strong></p> 
<p id="articleContentId"><strong><a href="https://blog.csdn.net/weixin_47723732/article/details/122636097?spm=1001.2014.3001.5501" title="机器学习之Python开源教程——专栏介绍及理论知识概述">机器学习之Python开源教程——专栏介绍及理论知识概述</a></strong></p> 
<p id="articleContentId"><strong><strong><a href="https://blog.csdn.net/weixin_47723732/article/details/122660071?spm=1001.2014.3001.5501" title="机器学习框架及评估指标详解">机器学习框架及评估指标详解</a></strong></strong></p> 
<p id="articleContentId"><strong><strong><a href="https://wxw-123.blog.csdn.net/article/details/122663642" title="Python监督学习之分类算法的概述">Python监督学习之分类算法的概述</a></strong></strong></p> 
<p id="articleContentId"><strong><strong><a href="https://wxw-123.blog.csdn.net/article/details/122667146" title="数据预处理之数据清理，数据集成，数据规约，数据变化和离散化">数据预处理之数据清理，数据集成，数据规约，数据变化和离散化</a></strong></strong></p> 
<p id="articleContentId"><strong><strong><a href="https://wxw-123.blog.csdn.net/article/details/122706251" title="特征工程之One-Hot编码、label-encoding、自定义编码">特征工程之One-Hot编码、label-encoding、自定义编码</a></strong></strong></p> 
<p id="articleContentId"><strong><strong><a href="https://wxw-123.blog.csdn.net/article/details/122709376" title="卡方分箱、KS分箱、最优IV分箱、树结构分箱、自定义分箱">卡方分箱、KS分箱、最优IV分箱、树结构分箱、自定义分箱</a></strong></strong></p> 
<p id="articleContentId"><strong><strong><a href="https://wxw-123.blog.csdn.net/article/details/122712340" title="特征选取之单变量统计、基于模型选择、迭代选择">特征选取之单变量统计、基于模型选择、迭代选择</a></strong></strong></p> 
<p id="articleContentId"><strong><strong><a href="https://wxw-123.blog.csdn.net/article/details/122743347" title="机器学习分类算法之朴素贝叶斯">机器学习分类算法之朴素贝叶斯</a></strong></strong></p> 
<p id="articleContentId"><strong><strong><a href="https://wxw-123.blog.csdn.net/article/details/122789729" title="【万字详解·附代码】机器学习分类算法之K近邻（KNN）">【万字详解·附代码】机器学习分类算法之K近邻（KNN）</a></strong></strong></p> 
<p id="articleContentId"><strong><strong><a href="https://wxw-123.blog.csdn.net/article/details/122797029" title="《全网最强》详解机器学习分类算法之决策树（附可视化和代码）">《全网最强》详解机器学习分类算法之决策树（附可视化和代码）</a></strong></strong></p> 
<p id="articleContentId"><strong><strong><a href="https://wxw-123.blog.csdn.net/article/details/122800504" title="机器学习分类算法之支持向量机">机器学习分类算法之支持向量机</a></strong></strong></p> 
<p><strong>持续更新中~</strong></p> 
<blockquote> 
 <p><strong>作者简介</strong></p> 
 <p><strong>博客名：</strong><strong>王小王-123</strong></p> 
 <p><strong>简介：</strong>CSDN博客专家、CSDN签约作者、华为云享专家，腾讯云、阿里云、简书、InfoQ创作者。公众号：书剧可诗画，2020年度CSDN优秀创作者。左手诗情画意，右手代码人生，欢迎一起探讨技术的诗情画意！</p> 
</blockquote> 
<hr>
<h2 id="%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E7%AE%97%E6%B3%95%E8%83%8C%E6%99%AF%E4%BB%8B%E7%BB%8D" style="margin-left:0in;text-align:left;"><a name="t1"></a><span style="color:#c00000;"><strong>支持向量机算法背景介绍</strong></span></h2> 
<p>1995年Cortes和Vapnik首先提出了<strong>支持向量机</strong>(<a href="https://so.csdn.net/so/search?q=Support&amp;spm=1001.2101.3001.7020" target="_blank" class="hl hl-1" data-report-click="{&quot;spm&quot;:&quot;1001.2101.3001.7020&quot;,&quot;dest&quot;:&quot;https://so.csdn.net/so/search?q=Support&amp;spm=1001.2101.3001.7020&quot;}">Support</a> Vector Machine)，由于其能够适应<strong>小样本的分类</strong>，<strong>分类速度快</strong>等特点，性能不差于<strong>人工神经网络</strong>，所以在这之后，人们将<strong>SVM</strong>应用于各个领域。</p> 
<p>大量使用<a href="https://so.csdn.net/so/search?q=SVM&amp;spm=1001.2101.3001.7020" target="_blank" class="hl hl-1" data-report-click="{&quot;spm&quot;:&quot;1001.2101.3001.7020&quot;,&quot;dest&quot;:&quot;https://so.csdn.net/so/search?q=SVM&amp;spm=1001.2101.3001.7020&quot;}">SVM</a>模型的论文不断涌现，包括国内和国外支持向量机建立在坚实的统计学理论基础上，是在所有知名的数据挖掘算法中最健壮、最准确的方法之一，具有很好的学习能力和泛化能力。</p> 
<h3 id="%E4%BB%80%E4%B9%88%E6%98%AF%E7%BA%BF%E6%80%A7%E5%8F%AF%E5%88%86%EF%BC%9F"><a name="t2"></a>什么是<a href="https://so.csdn.net/so/search?q=%E7%BA%BF%E6%80%A7&amp;spm=1001.2101.3001.7020" target="_blank" class="hl hl-1" data-report-click="{&quot;spm&quot;:&quot;1001.2101.3001.7020&quot;,&quot;dest&quot;:&quot;https://so.csdn.net/so/search?q=%E7%BA%BF%E6%80%A7&amp;spm=1001.2101.3001.7020&quot;}">线性</a>可分？</h3> 
<blockquote> 
 <p style="margin-left:0in;text-align:left;"><span style="color:#000000;">简单说，线性可分就是在多维空间中存在一个超平面（</span><span style="color:#000000;">Hyper Plane</span><span style="color:#000000;">），可以把样例数据清楚地分成不同类别。二分类就是最典型的线性可分问题。</span></p> 
 <p style="text-align:center;"><img alt="" height="302" src="https://img-blog.csdnimg.cn/3b5b43b7d3dc47669c5627e860bb137f.png" width="391"></p> 
</blockquote> 
<h3 id="%E4%BB%80%E4%B9%88%E5%8F%88%E6%98%AF%E8%B6%85%E5%B9%B3%E9%9D%A2%EF%BC%9F" style="margin-left:0in;text-align:left;"><a name="t3"></a>什么又是超平面？</h3> 
<p style="margin-left:0in;text-align:left;"><span style="color:#000000;">超平面</span><span style="color:#000000;">H</span><span style="color:#000000;">是从</span><span style="color:#000000;">n</span><span style="color:#000000;">维空间到</span><span style="color:#000000;">n-1</span><span style="color:#000000;">维空间的一个映射子空间，它有一个</span><span style="color:#000000;">n</span><span style="color:#000000;">维向量和一个实数定义。因为是子空间，所以超平面一定过原点。</span></p> 
<blockquote> 
 <p style="text-align:center;"><img alt="" height="247" src="https://img-blog.csdnimg.cn/a350a6246df64dc19f09e238c15dbc13.png" width="520"></p> 
 <p><strong>利用这种方式，我们可以看到即使再多维的情况下，也可以生成一种超平面用来进行分类</strong></p> 
</blockquote> 
<p><strong>怎么正确理解超平面？</strong></p> 
<p>超平面是个纯数学概念，不是物理概念。它是直线中的点、平面中的直线、空间中的平面的推广，只有当维度大于3，才能称为“超”平面。<br> 超平面的本质是<strong>自由度比空间维度小1</strong>，也即<strong>最后一个维度可以因其他维度确定而确定</strong>。</p> 
<p><strong>超平面的两个性质：</strong></p> 
<p>1）方程是线性的: 是空间点的各分量的线性组合<br> 2）方程数量为1</p> 
<blockquote> 
 <p style="text-align:center;"><img alt="" height="362" src="https://img-blog.csdnimg.cn/9f25fea78e93459b850f0fa8c5cacf4d.png" width="957"></p> 
</blockquote> 
<p><strong>说到这里，可能还是比较的懵逼，到底什么是超平面，超平面又该如何选择呢？我们可以看看下面的这个例子！</strong></p> 
<blockquote> 
 <p style="margin-left:0in;text-align:left;"><span style="color:#000000;">下面是一个二分类的问题，对于蓝点和黑点数据，事实上存在很多条直线可以把他们正确地划分开。</span><span style="color:#000000;">那么究竟选择哪一条直线最佳呢？</span></p> 
 <p style="text-align:center;"><img alt="" height="244" src="https://img-blog.csdnimg.cn/8d90294a149e4a1a9f5bfcc6d6544bef.png" width="424"></p> 
</blockquote> 
<blockquote> 
 <p style="margin-left:0in;text-align:left;"><span style="color:#000000;">我们发现存在三条特别的超平面</span><span style="color:#000000;">(</span><span style="color:#000000;">直线</span><span style="color:#000000;">)</span><span style="color:#000000;">。两条紫色的线分别距离蓝点和黑点最近，而红色的线则和两条紫线的距离相等。我们把两条紫线的距离称作间隔</span><span style="color:#000000;">Margin</span></p> 
 <p style="text-align:center;"><img alt="" height="271" src="https://img-blog.csdnimg.cn/769ba8905426498d96638227d9755103.png" width="367"></p> 
</blockquote> 
<blockquote> 
 <p style="margin-left:0in;text-align:left;"><span style="color:#000000;">假如我们设定 </span><span style="color:#000000;">g(X)=W</span><span style="color:#000000;">T</span><span style="color:#000000;">X+b</span><span style="color:#000000;">为线性判别函数。对</span><span style="color:#000000;">g(X)</span><span style="color:#000000;">进行归一化，即令</span><span style="color:#000000;">g</span><span style="color:#000000;">1</span><span style="color:#000000;">(X)=g(X)/c,</span><span style="color:#000000;">也即</span><span style="color:#000000;">W</span><span style="color:#000000;">1</span><span style="color:#000000;">=W</span><span style="color:#000000;">T</span><span style="color:#000000;"> /c</span><span style="color:#000000;">，</span><span style="color:#000000;">b</span><span style="color:#000000;">1</span><span style="color:#000000;">=b/c</span><span style="color:#000000;">，我们可以得到新的左图。</span></p> 
 <p style="text-align:center;"><img alt="" height="105" src="https://img-blog.csdnimg.cn/9d52f8f3516c4019a1fe2da9eec898d4.png" width="447"></p> 
 <p style="text-align:center;"><img alt="" height="262" src="https://img-blog.csdnimg.cn/d7e19ec3859448a58fc23f4cffc2ad6f.png" width="348"></p> 
 <p><strong>这也是，为什么支持向量机需要使用归一化，可以大幅度的提高模型的效果的原因</strong></p> 
</blockquote> 
<p><strong><span style="color:#c00000;">约束条件合并</span></strong></p> 
<p><strong>对于上图而言：</strong></p> 
<p style="margin-left:0in;text-align:left;"><span style="color:#000000;">1</span><span style="color:#000000;">）蓝色样本点</span><span style="color:#000000;">(y=+1)</span><span style="color:#000000;">必定在直线</span><span style="color:#000000;">W</span><span style="color:#000000;">1</span><span style="color:#000000;">T</span><span style="color:#000000;">X+b</span><span style="color:#000000;">1</span><span style="color:#000000;">=+1</span><span style="color:#000000;">上方，也即对所有蓝色样本点数据必定满足：</span><span style="color:#000000;"> W</span><span style="color:#000000;">1</span><span style="color:#000000;">T</span><span style="color:#000000;">X+b</span><span style="color:#000000;">1</span><span style="color:#000000;">≥+</span><span style="color:#000000;">1</span></p> 
<p style="margin-left:0in;text-align:left;"><span style="color:#000000;">2</span><span style="color:#000000;">）黑色样本点</span><span style="color:#000000;">(y=-1)</span><span style="color:#000000;">必定在直线</span><span style="color:#000000;">W</span><span style="color:#000000;">1</span><span style="color:#000000;">T</span><span style="color:#000000;">X+b</span><span style="color:#000000;">1</span><span style="color:#000000;">=-1</span><span style="color:#000000;">下面，也即对所有黑色样本点数据必定满足：</span><span style="color:#000000;"> W</span><span style="color:#000000;">1</span><span style="color:#000000;">T</span><span style="color:#000000;">X+b</span><span style="color:#000000;">1</span><span style="color:#000000;">≤</span><span style="color:#000000;">-1</span></p> 
<p style="margin-left:0in;text-align:left;"><strong><span style="color:#000000;">我们把以上两个不等式合成一个，即：</span></strong></p> 
<p><img alt="" height="109" src="https://img-blog.csdnimg.cn/5db9531c39af4f8299e270f7b1db1eb6.png" width="977"></p> 
<blockquote> 
 <p></p> 
 <p style="text-align:center;"><img alt="" height="261" src="https://img-blog.csdnimg.cn/363a99269afe4bd3a670c7c2f9c0712c.png" width="374"></p> 
 <p style="margin-left:0in;text-align:left;"><strong><span style="color:#000000;">距离超平面最近的这几个样本点满足</span><span style="color:#000000;">y</span><span style="color:#000000;">i</span><span style="color:#000000;">(W</span><span style="color:#000000;">T</span><span style="color:#000000;">x</span><span style="color:#000000;">i</span><span style="color:#000000;">+b)=1</span><span style="color:#000000;">，它们被称为“支持向量”。虚线称为边界，两条虚线间的距离称为间隔（</span><span style="color:#000000;">margin</span><span style="color:#000000;">）。而间隔</span><span style="color:#000000;">(margin)</span><span style="color:#000000;">其实就是两个异类支持向量的差在法向量</span><span style="color:#000000;">W</span><span style="color:#000000;">方向的投影，故有：</span></strong></p> 
 <p style="text-align:center;"><img alt="" height="97" src="https://img-blog.csdnimg.cn/5e57a618a05f4b71ad5a303e68007874.png" width="606"></p> 
</blockquote> 
<p><strong>经过了大量的铺垫工作之后，现在我们对支持向量机进行一个定义</strong></p> 
<p style="margin-left:0in;text-align:left;"><span style="color:#000000;">支持向量机（</span><span style="color:#000000;">support vector machines</span><span style="color:#000000;">）是一种二分类模型，它的目的是寻找一个超平面来对样本进行分割，分割的原则是间隔最大化，最终转化为一个凸二次规划问题来求解：</span></p> 
<p style="margin-left:0in;text-align:left;"><img alt="" height="223" src="https://img-blog.csdnimg.cn/4bfb8643005141b3b97127346b517efa.png" width="1048"></p> 
<h3 id="%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E7%9A%84%E4%B8%89%E7%A7%8D%E6%83%85%E5%86%B5"><a name="t4"></a><a href="https://so.csdn.net/so/search?q=%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA&amp;spm=1001.2101.3001.7020" target="_blank" class="hl hl-1" data-report-view="{&quot;spm&quot;:&quot;1001.2101.3001.7020&quot;,&quot;dest&quot;:&quot;https://so.csdn.net/so/search?q=%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA&amp;spm=1001.2101.3001.7020&quot;}" data-report-click="{&quot;spm&quot;:&quot;1001.2101.3001.7020&quot;,&quot;dest&quot;:&quot;https://so.csdn.net/so/search?q=%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA&amp;spm=1001.2101.3001.7020&quot;}">支持向量机</a>的三种情况</h3> 
<p><strong><span style="color:#c00000;">线性可分</span></strong></p> 
<p style="margin-left:0in;text-align:left;"><span style="color:#000000;">线性可分指样本数据集可以找到一个线性函数或者超平面一分为二，这样的支持向量机又叫硬间隔</span><span style="color:#000000;">(Hard Margin)</span><span style="color:#000000;">支持向量机。求解过程如下：</span></p> 
<blockquote> 
 <p style="margin-left:0in;text-align:left;"><span style="color:#000000;">1</span><span style="color:#000000;">）使用拉格朗日乘子法得到其对偶问题：</span></p> 
 <p style="margin-left:0in;text-align:left;"><img alt="" height="82" src="https://img-blog.csdnimg.cn/c9ec43d900d74f75bba2a6a7a44b75e0.png" width="700"></p> 
 <p><span style="color:#000000;">2</span><span style="color:#000000;">）分别对</span><span style="color:#000000;">W</span><span style="color:#000000;">和</span><span style="color:#000000;">b</span><span style="color:#000000;">求偏导，并令其等于</span><span style="color:#000000;">0</span><span style="color:#000000;">：</span></p> 
 <p><img alt="" height="176" src="https://img-blog.csdnimg.cn/be6ef9399b654f2fb53c80d0911b9f0e.png" width="1050">&nbsp;<span style="color:#000000;">3</span><span style="color:#000000;">）将</span><span style="color:#000000;">(9)</span><span style="color:#000000;">和</span><span style="color:#000000;">(10)</span><span style="color:#000000;">代回公式</span><span style="color:#000000;">(8)</span><span style="color:#000000;">，消去之前</span><span style="color:#000000;">W</span><span style="color:#000000;">和</span><span style="color:#000000;">b</span><span style="color:#000000;">，原问题就转换成了关于</span><span style="color:#000000;">α</span><span style="color:#000000;">的问题</span></p> 
 <p><img alt="" height="200" src="https://img-blog.csdnimg.cn/56fd01c0174a470ea248b626293df091.png" width="794">&nbsp;<span style="color:#000000;">4</span><span style="color:#000000;">）新的目标函数变成如下：</span></p> 
 <p style="text-align:center;"><img alt="" height="79" src="https://img-blog.csdnimg.cn/70a910f3ad724938a8ab3309601b3c8d.png" width="321"></p> 
 <p><span style="color:#000000;">5</span><span style="color:#000000;">）这样解出</span><span style="color:#000000;">α</span><span style="color:#000000;">之后，就可以根据公式</span><span style="color:#000000;">(9)</span><span style="color:#000000;">求</span><span style="color:#000000;">W</span><span style="color:#000000;">，进而求偏移量</span><span style="color:#000000;">b</span></p> 
 <p style="text-align:center;"><img alt="" height="65" src="https://img-blog.csdnimg.cn/f899b217cd6f4f98913ea1cd687d2033.png" width="461"></p> 
 <p><span style="color:#000000;">6</span><span style="color:#000000;">）该过程的</span><span style="color:#000000;">KKT</span><span style="color:#000000;">条件为：</span></p> 
 <p style="text-align:center;"><img alt="" height="82" src="https://img-blog.csdnimg.cn/8becffdf4ada427f9210a750afad4001.png" width="144"></p> 
 <p>对于任意的训练样本 (xi,yi)：<br> 若 αi=0,则其不会在公式（13）中的求和项中出现，也就是说，它不影响<a href="https://so.csdn.net/so/search?q=%E6%A8%A1%E5%9E%8B&amp;spm=1001.2101.3001.7020" target="_blank" class="hl hl-1" data-report-view="{&quot;spm&quot;:&quot;1001.2101.3001.7020&quot;,&quot;dest&quot;:&quot;https://so.csdn.net/so/search?q=%E6%A8%A1%E5%9E%8B&amp;spm=1001.2101.3001.7020&quot;}" data-report-click="{&quot;spm&quot;:&quot;1001.2101.3001.7020&quot;,&quot;dest&quot;:&quot;https://so.csdn.net/so/search?q=%E6%A8%A1%E5%9E%8B&amp;spm=1001.2101.3001.7020&quot;}">模型</a>的训练；<br> 若 αi&gt;0，则yif(xi)−1=0，也就是 yif(xi)=1，即该样本一定在边界上，是一个支持向量。</p> 
 <p><span style="color:#fe2c24;"><strong>这里显示出了支持向量机的重要特征：当训练完成后，大部分样本都不需要保留，最终模型只与支持向量有关。</strong></span></p> 
</blockquote> 
<h3 id="%E8%BF%91%E7%BA%BF%E6%80%A7%E5%8F%AF%E5%88%86"><a name="t5"></a>近线性可分</h3> 
<blockquote> 
 <p style="margin-left:0in;text-align:left;"><span style="color:#000000;">假如数据本身并非线性不可分的，只是因为噪声数据的存在，一些偏离正常位置很远的离群点数据（</span><span style="color:#000000;">Outlier</span><span style="color:#000000;">），使得数据线性不可分。</span></p> 
 <p style="margin-left:0in;text-align:left;"><span style="color:#000000;">为解决这一问题，可以对每个样本点引入一个松弛变量 </span><span style="color:#000000;">ξi≥0</span><span style="color:#000000;">，使得间隔加上松弛变量大于等于</span><span style="color:#000000;">1</span><span style="color:#000000;">，这样处理后数据仍然可以实现线性可分，我们称之为近线性可分，这样的支持向量机也叫软间隔</span><span style="color:#000000;">(Soft Margin)</span><span style="color:#000000;">支持向量机。</span></p> 
 <p style="margin-left:0in;text-align:left;"><img alt="" height="247" src="https://img-blog.csdnimg.cn/1bbc4f6e3e9347da8658c3169ef69e40.png" width="701"></p> 
 <p><span style="color:#000000;">这个时候，原问题的约束条件变成了：</span>&nbsp;</p> 
 <p style="text-align:center;"><img alt="" height="36" src="https://img-blog.csdnimg.cn/b18e449151ba4438a34599bbf9cda039.png" width="273"></p> 
 <p>&nbsp;<span style="color:#000000;">同时，对于每一个松弛变量</span><span style="color:#000000;">ξi≥0</span><span style="color:#000000;">，支付一个代价 </span><span style="color:#000000;">ξi≥0</span><span style="color:#000000;">，目标函数变为：</span></p> 
 <p style="text-align:center;"><img alt="" height="59" src="https://img-blog.csdnimg.cn/4e2d29d8e9084fe1ae9625ba8e236c15.png" width="217"></p> 
 <p style="margin-left:0in;text-align:left;"><span style="color:#000000;">其中 </span><span style="color:#000000;">C&gt;0</span><span style="color:#000000;">为惩罚参数，</span><span style="color:#000000;">C</span><span style="color:#000000;">值大时对误分类的惩罚增大， </span><span style="color:#000000;">C</span><span style="color:#000000;">值小时对误分类的惩罚减小，公式（</span><span style="color:#000000;">21</span><span style="color:#000000;">）包含两层含义：使 </span><span style="color:#000000;">||W||</span><span style="color:#000000;">2</span><span style="color:#000000;">/2</span><span style="color:#000000;">尽量小即间隔尽量大，<strong>同时使误分类点的个数尽量小，</strong></span><strong><span style="color:#000000;">C</span><span style="color:#000000;">是调和两者的系数。 </span></strong></p> 
 <p style="margin-left:0in;text-align:left;">&nbsp;<span style="color:#000000;">有了公式（21），可以和线性可分支持向量机一样考虑线性支持向量机的学习过程</span></p> 
</blockquote> 
<p><strong>&nbsp;近线性可分求解过程</strong>&nbsp;</p> 
<blockquote> 
 <p>&nbsp;<span style="color:#000000;">1</span><span style="color:#000000;">）近线性支持向量机的学习问题变成如下凸二次规划问题的求解（原始问题）：</span></p> 
 <p><img alt="" height="93" src="https://img-blog.csdnimg.cn/26865cd1722241e989cc7765f67a5198.png" width="967"></p> 
 <p><span style="color:#000000;">2</span><span style="color:#000000;">）对其使用拉格朗日函数，利用对偶问题求解，公式（</span><span style="color:#000000;">22</span><span style="color:#000000;">）的拉格朗日函数为：</span>&nbsp;</p> 
 <p><img alt="" height="83" src="https://img-blog.csdnimg.cn/a7ea199f51a7467e84b9533c0e445c10.png" width="1064"></p> 
 <p>&nbsp;<span style="color:#000000;">其中 </span><span style="color:#000000;">α</span><span style="color:#000000;">i≥0,</span><span style="color:#000000;">μ</span><span style="color:#000000;">i≥0</span><span style="color:#000000;">是拉格朗日乘子。</span></p> 
 <p style="margin-left:0in;text-align:left;"><span style="color:#000000;">3</span><span style="color:#000000;">）令</span><span style="color:#000000;">L(</span><span style="color:#000000;">w,b</span><span style="color:#000000;">,</span><span style="color:#000000;">α,ξ,μ)</span><span style="color:#000000;">对</span><span style="color:#000000;">w,b</span><span style="color:#000000;">,</span><span style="color:#000000;">ξ</span><span style="color:#000000;">的偏导数为</span><span style="color:#000000;">0</span><span style="color:#000000;">可得如下</span><span style="color:#000000;">:</span></p> 
 <p style="text-align:center;"><img alt="" height="141" src="https://img-blog.csdnimg.cn/e161c0932f97439697106c5c6d9f7169.png" width="178"></p> 
 <p>&nbsp;<span style="color:#000000;">4</span><span style="color:#000000;">）将公式（</span><span style="color:#000000;">24</span><span style="color:#000000;">）（</span><span style="color:#000000;">25</span><span style="color:#000000;">）（</span><span style="color:#000000;">26</span><span style="color:#000000;">）代入公式（</span><span style="color:#000000;">23</span><span style="color:#000000;">）得对偶问题：</span></p> 
 <p><img alt="" height="192" src="https://img-blog.csdnimg.cn/56d8caa27d9848ea9ef9496e911b34ca.png" width="906"></p> 
 <p><span style="color:#000000;">5</span><span style="color:#000000;">）上述过程的</span><span style="color:#000000;">KKT</span><span style="color:#000000;">条件为：</span></p> 
 <p style="text-align:center;"><img alt="" height="106" src="https://img-blog.csdnimg.cn/a9b5a27113b4489a9e13c6adb1fd8229.png" width="234"></p> 
 <p></p> 
 <p style="margin-left:0in;text-align:left;"><strong><span style="color:#000000;">对于任意的训练样本 (x</span><span style="color:#000000;">i</span><span style="color:#000000;">,y</span><span style="color:#000000;">i</span><span style="color:#000000;">) ，总有 </span><span style="color:#000000;">α</span><span style="color:#000000;">i</span><span style="color:#000000;">=0 </span><span style="color:#000000;">或者</span><span style="color:#000000;">y</span><span style="color:#000000;">i</span><span style="color:#000000;">f</span><span style="color:#000000;">(x</span><span style="color:#000000;">i</span><span style="color:#000000;">)−1+ξ</span><span style="color:#000000;">i</span><span style="color:#000000;">=0</span><span style="color:#000000;">：</span></strong></p> 
 <div style="margin-left:.38in;text-align:left;">
  <strong>1)<span style="color:#000000;">若 α</span><span style="color:#000000;">i</span><span style="color:#000000;">=0</span><span style="color:#000000;">,</span><span style="color:#000000;">则其不会在公式（13）中的求和项中出现，也就是说，它不影响模型的训练；</span></strong>
 </div> 
 <div style="margin-left:.38in;text-align:left;">
  <strong>2)<span style="color:#000000;">若 α</span><span style="color:#000000;">i</span><span style="color:#000000;">&gt;0，则y</span><span style="color:#000000;">i</span><span style="color:#000000;">f(x</span><span style="color:#000000;">i</span><span style="color:#000000;">)−1</span><span style="color:#000000;">+ξ</span><span style="color:#000000;">i</span><span style="color:#000000;">=0，也就是 y</span><span style="color:#000000;">i</span><span style="color:#000000;">f(x</span><span style="color:#000000;">i</span><span style="color:#000000;">)=1</span><span style="color:#000000;">-</span><span style="color:#000000;"> ξ</span><span style="color:#000000;">i</span> <span style="color:#000000;">，即该样本一定在边界上，是一个支持向量。</span></strong>
 </div> 
 <p style="margin-left:0in;text-align:left;"><span style="color:#ff0000;">结果跟线性可分</span><span style="color:#ff0000;">SVM</span><span style="color:#ff0000;">差不多，这个也是</span><span style="color:#ff0000;">SVM</span><span style="color:#ff0000;">算法的一大特色</span></p> 
</blockquote> 
<h3 id="%E7%BA%BF%E6%80%A7%E4%B8%8D%E5%8F%AF%E5%88%86"><a name="t6"></a>线性不可分</h3> 
<blockquote> 
 <p style="margin-left:0in;text-align:left;"><span style="color:#000000;">如果数据确实线性不可分，这个时候可以考虑把数据映射到更高维的空间，然后使得转换后的数据变得线性可分。</span></p> 
 <p style="margin-left:0in;text-align:left;"><span style="color:#000000;">这个过程中，会引入一个</span><span style="color:#000000;"><strong>核函数</strong></span><span style="color:#000000;">的概念，具体来说，在线性不可分的情况下，支持向量机首先在低维空间中完成计算，然后通过核函数将输入空间映射到高维特征空间，最终在高维特征空间中构造出<strong>最优分离超平面</strong>，从而把平面上本身不好分的非线性数据分开。</span></p> 
 <p style="margin-left:0in;text-align:left;">通俗易懂的来讲就是在多维空间里面利用多维曲面进行分类</p> 
 <p style="text-align:center;"><img alt="" height="166" src="https://img-blog.csdnimg.cn/7d4eec6677a945f6a9a9936561ec78e4.png" width="347"></p> 
 <h3 id="%E4%B8%8D%E7%94%A8%E6%A0%B8%E5%87%BD%E6%95%B0%E7%9A%84%E4%BC%A0%E7%BB%9F%E6%96%B9%E6%B3%95"><a name="t7"></a>不用核函数的传统方法</h3> 
 <p style="margin-left:0in;text-align:left;"><span style="color:#000000;">如果用原始的方法，那么在用线性学习器学习一个非线性关系，需要选择一个非线性特征集，并且将数据写成新的表达形式，这等价于应用一个固定的非线性映射，将数据映射到特征空间，在特征空间中使用线性学习器，因此，考虑的假设集是这种类型的函数：</span></p> 
 <p style="text-align:center;"><img alt="" height="65" src="https://img-blog.csdnimg.cn/02390759270b4e54b7b9c54b0b36f5ee.png" width="186"></p> 
 <p style="margin-left:0in;text-align:left;"><strong><span style="color:#000000;">这里</span><span style="color:#000000;">ϕ</span><span style="color:#000000;">：</span><span style="color:#000000;">X-&gt;F</span><span style="color:#000000;">是从输入空间到某个特征空间的映射，这意味着建立非线性学习器分为两步：</span></strong></p> 
 <p style="margin-left:0in;text-align:left;"><strong><span style="color:#000000;">1</span><span style="color:#000000;">）首先使用一个非线性映射将数据变换到一个特征空间</span><span style="color:#000000;">F</span><span style="color:#000000;">，</span></strong></p> 
 <p style="margin-left:0in;text-align:left;"><strong><span style="color:#000000;">2</span><span style="color:#000000;">）然后在特征空间使用线性学习器分类。</span></strong></p> 
 <p style="margin-left:0in;text-align:left;"><strong><span style="color:#000000;">但这种方法随着维度的增长，存在很大的问题：</span></strong></p> 
 <p style="margin-left:0in;text-align:left;"><span style="color:#000000;">1</span><span style="color:#000000;">）我们对一个二维空间构造一个非线性的曲面做映射，选择的新空间是原始空间的所有一阶和二阶的组合，那么会得到五个维度；</span></p> 
 <p style="margin-left:0in;text-align:left;"><img alt="" height="70" src="https://img-blog.csdnimg.cn/05a3d744c1944f8aa885076099c2ab1f.png" width="838"></p> 
 <p><span style="color:#000000;">2</span><span style="color:#000000;">）如果原始空间是三维（一阶、二阶和三阶的组合），那么我们会得到：</span><span style="color:#000000;">3(</span><span style="color:#000000;">一次</span><span style="color:#000000;">)+3(</span><span style="color:#000000;">二次交叉</span><span style="color:#000000;">)+3(</span><span style="color:#000000;">平方</span><span style="color:#000000;">)+3(</span><span style="color:#000000;">立方</span><span style="color:#000000;">)+1(x1*x2*x3)+2*3(</span><span style="color:#000000;">交叉，一个一次一个二次，类似</span><span style="color:#000000;">x1*x2^2) = 19</span><span style="color:#000000;">维的新空间，这个数目是呈指数级爆炸性增长的从而势必这给的计算带来非常大的困难。</span></p> 
</blockquote> 
<p style="margin-left:0in;text-align:left;"><strong><span style="color:#000000;">如果遇到无穷维的情况，就更加无从计算了。这个时候我们就必须引入</span><span style="color:#000000;">Kernel</span><span style="color:#000000;">核函数</span></strong></p> 
<h3 id="%E6%A0%B8%E5%87%BD%E6%95%B0Kernel%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><a name="t8"></a>核函数<a href="https://so.csdn.net/so/search?q=Kernel&amp;spm=1001.2101.3001.7020" target="_blank" class="hl hl-1" data-report-view="{&quot;spm&quot;:&quot;1001.2101.3001.7020&quot;,&quot;dest&quot;:&quot;https://so.csdn.net/so/search?q=Kernel&amp;spm=1001.2101.3001.7020&quot;}" data-report-click="{&quot;spm&quot;:&quot;1001.2101.3001.7020&quot;,&quot;dest&quot;:&quot;https://so.csdn.net/so/search?q=Kernel&amp;spm=1001.2101.3001.7020&quot;}">Kernel</a>是什么？</h3> 
<blockquote> 
 <p style="margin-left:0in;text-align:left;"><span style="color:#000000;">核是一个</span><span style="color:#c00000;">特征空间的隐式映射</span><span style="color:#000000;">，比如假定</span><span style="color:#000000;">函数</span><span style="color:#000000;">K</span><span style="color:#000000;">，对所有</span><span style="color:#000000;">x</span><span style="color:#000000;">，</span><span style="color:#000000;">z(-X</span><span style="color:#000000;">，满足</span></p> 
 <p style="text-align:center;"><img alt="" height="41" src="https://img-blog.csdnimg.cn/ec0a28be2d104892a33e0fbd44e43f97.png" width="258"></p> 
 <p style="text-align:center;"><img alt="" height="235" src="https://img-blog.csdnimg.cn/26702a2612714737831bf7c7ddc818d5.png" width="638"></p> 
 <p>&nbsp;<strong>核函数与传统的计算方式不同之处：</strong></p> 
 <p style="margin-left:0in;text-align:left;"><span style="color:#000000;">1</span><span style="color:#000000;">、</span><span style="color:#000000;">(</span><span style="color:#000000;">传统</span><span style="color:#000000;">)</span><span style="color:#000000;">一个是<strong>映射到高维空间中，然后再根据内积的公式进行计算</strong>；</span></p> 
 <p style="margin-left:0in;text-align:left;"><span style="color:#000000;">2</span><span style="color:#000000;">、</span><span style="color:#000000;">(</span><span style="color:#000000;">核函数</span><span style="color:#000000;">)</span><span style="color:#000000;"> <strong>则直接在原来的低维空间中进行计算，而不需要显式地写出映射后的结果。</strong></span></p> 
 <h3 id="%E6%A0%B8%E5%87%BD%E6%95%B0SVM%E6%B1%82%E8%A7%A3%E8%BF%87%E7%A8%8B"><a name="t9"></a>核函数SVM求解过程</h3> 
 <p style="margin-left:0in;text-align:left;"><span style="color:#000000;">1</span><span style="color:#000000;">）令</span><span style="color:#000000;">ϕ(x)\phi (x)ϕ(x)</span><span style="color:#000000;">表示将 </span><span style="color:#000000;">x </span><span style="color:#000000;">映射后的特征向量，于是划分超平面所对应的的模型：</span></p> 
 <p style="text-align:center;"><img alt="" height="26" src="https://img-blog.csdnimg.cn/64c39df6601c4984ad660012c8d3b9e0.png" width="282"></p> 
 <p>&nbsp;<span style="color:#000000;">2</span><span style="color:#000000;">）于是有最小化函数：</span></p> 
 <p style="text-align:center;"><img alt="" height="59" src="https://img-blog.csdnimg.cn/3e1410ded5f94032a516cc4a1d893ee8.png" width="547"></p> 
 <p><span style="color:#000000;">3</span><span style="color:#000000;">）利用拉格朗日乘子法，求对偶问题</span><span style="color:#000000;">:</span></p> 
 <p style="text-align:center;"><img alt="" height="110" src="https://img-blog.csdnimg.cn/2726a668a53e4e9e9f7e89c2cdfad89b.png" width="361"></p> 
 <p>&nbsp;<span style="color:#000000;">4</span><span style="color:#000000;">）若要对公式（</span><span style="color:#000000;">16</span><span style="color:#000000;">）求解，会涉及到计算 </span><span style="color:#000000;">ϕ(xi)</span><span style="color:#000000;">T</span><span style="color:#000000;">ϕ(</span><span style="color:#000000;">xj</span><span style="color:#000000;">) </span><span style="color:#000000;">之后的内积，由于特征空间的维数可能很高，甚至是无穷维，因此直接计算 </span><span style="color:#000000;">ϕ(xi)</span><span style="color:#000000;">T</span><span style="color:#000000;">ϕ(</span><span style="color:#000000;">xj</span><span style="color:#000000;">)</span><span style="color:#000000;">通常是困难的，于是想到这样一个函数：</span></p> 
 <p style="text-align:center;"><img alt="" height="34" src="https://img-blog.csdnimg.cn/e2a922fd006d4b18aef2aec64d07ddc3.png" width="366"></p> 
 <p>&nbsp;<span style="color:#000000;">即 </span><span style="color:#000000;">xi</span><span style="color:#000000;">和</span><span style="color:#000000;">xj</span><span style="color:#000000;">在特征空间中的内积等于他们在原始样本空间中通过函数 </span><span style="color:#000000;">κ(</span><span style="color:#000000;">xi,xj</span><span style="color:#000000;">) </span><span style="color:#000000;">计算的函数值，于是公式（</span><span style="color:#000000;">16</span><span style="color:#000000;">）写成如下：</span></p> 
 <p style="text-align:center;"><img alt="" height="88" src="https://img-blog.csdnimg.cn/f63dc14672c84c068474129ceb6dec18.png" width="277"></p> 
 <p></p> 
 <p>&nbsp;<strong>最后得到</strong></p> 
 <p style="text-align:center;"><img alt="" height="127" src="https://img-blog.csdnimg.cn/33789e2bbd0b44d4ac7997bed6aff7df.png" width="320"></p> 
 <p style="margin-left:0in;text-align:left;"><strong><span style="color:#000000;">其中</span><span style="color:#000000;">k(</span><span style="color:#000000;">x</span><span style="color:#000000;">i</span><span style="color:#000000;">,x</span><span style="color:#000000;">j</span><span style="color:#000000;">)</span><span style="color:#000000;">就是核函数，可以根据情况有多种选择</span></strong></p> 
</blockquote> 
<p><img alt="" height="326" src="https://img-blog.csdnimg.cn/7c46562241c74b3baf2c80a27e307f55.png" width="1136"></p> 
<h3 id="%E6%A0%B8%E5%87%BD%E6%95%B0%E7%9A%84%E6%9C%AC%E8%B4%A8"><a name="t10"></a>核函数的本质</h3> 
<p style="margin-left:0in;text-align:left;"><span style="color:#000000;">1</span><span style="color:#000000;">、实际中，我们会经常遇到线性不可分的样例，此时，我们的常用做法是把样例特征映射到高维空间中去</span><span style="color:#000000;">(</span><span style="color:#000000;">如之前那幅图线性不可分图所示，映射到高维空间后，相关特征便被分开了，也就达到了分类的目的</span><span style="color:#000000;">)</span><span style="color:#000000;">；</span></p> 
<p style="margin-left:0in;text-align:left;"><span style="color:#000000;">2</span><span style="color:#000000;">、但进一步，如果凡是遇到线性不可分的样例，一律映射到高维空间，那么这个维度大小是会高到可怕的</span><span style="color:#000000;">(</span><span style="color:#000000;">如上文中</span><span style="color:#000000;">19</span><span style="color:#000000;">维乃至无穷维的例子</span><span style="color:#000000;">)</span><span style="color:#000000;">。那咋办呢？</span></p> 
<p style="margin-left:0in;text-align:left;"><span style="color:#000000;">3</span><span style="color:#000000;">、此时，核函数就隆重登场了，核函数的价值在于它虽然也是将特征进行从低维到高维的转换，但核函数绝就绝在它事先在低维上进行计算，而将实质上的分类效果表现在了高维上，也就如上文所说的避免了直接在高维空间中的复杂计算。</span></p> 
<p style="margin-left:0in;text-align:left;"><strong>说到这里，相信差不多应该可以理解了吧，下面再去举一个例子</strong></p> 
<blockquote> 
 <p style="margin-left:0in;text-align:left;"><span style="color:#000000;">假设现在你是一个农场主，圈养了一批羊群，但为预防狼群袭击羊群，你需要搭建一个篱笆来把羊群围起来。但是篱笆应该建在哪里呢？你很可能需要依据牛群和狼群的位置建立一个“分类器”，比较下图这几种不同的分类器，我们可以看到</span><span style="color:#000000;">SVM</span><span style="color:#000000;">完成了一个很完美的解决方案。</span></p> 
 <p style="margin-left:0in;text-align:left;"><img alt="" height="480" src="https://img-blog.csdnimg.cn/897fbd2781114459972908eaeea39bc7.png" width="762"></p> 
 <p><strong>显然，我们的SVM胜利了！</strong></p> 
</blockquote> 
<p>&nbsp;<strong>讲了这么多，其实就是在介绍支持向量机的本质，如果你认真的看完之后，你会发现，在支持向量机里面，最重要的两个参数就是C和核函数</strong></p> 
<p><strong>支持向量机的优势在于：</strong><br> （ 1） 在高维空间中非常高效。<br> （ 2） 即使在数据维度比样本数量大的情况下仍然有效。<br> （ 3） 在决策函数（称为支持向量）中使用训练集的子集,因此它也是高效利用内存的。<br><strong>支持向量机的缺点包括：</strong><br> （ 1） 如果特征数量比样本数量大得多， 在选择核函数核函数时要避免过拟合。<br> （ 2） 而且正则化项是非常重要的。<br> （ 3） 支持向量机不直接提供概率估计， 这些都是使用昂贵的五次交叉验算计算的。<br><img alt="" height="344" src="https://img-blog.csdnimg.cn/e31cd072946745d6b0590d4bc0adc63f.png" width="783"></p> 
<p><strong>注意，这里很重要的，根据你选择的核函数，最终对数据是否需要预处理，下面我们就看看一个实例：</strong></p> 
<h2 id="%E4%BB%A3%E7%A0%81%E5%AE%9E%E4%BE%8B"><a name="t11"></a><strong>代码实例</strong></h2> 
<p><strong>导入第三方库</strong></p> 
<pre><code class="language-python hljs"><ol class="hljs-ln" style="width:100%"><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="1"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-comment">#导入所需要的包</span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="2"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> precision_score</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="3"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="4"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-keyword">from</span> sklearn.svm <span class="hljs-keyword">import</span> SVC</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="5"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="6"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="7"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="8"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-keyword">from</span> matplotlib.colors <span class="hljs-keyword">import</span> ListedColormap</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="9"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> LabelEncoder</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="10"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> classification_report</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="11"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> GridSearchCV <span class="hljs-comment">#网格搜索</span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="12"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<span class="hljs-comment">#可视化</span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="13"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> sns<span class="hljs-comment">#绘图包</span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="14"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> StandardScaler,MinMaxScaler,MaxAbsScaler</div></div></li></ol></code><div class="hljs-button signin" data-title="登录后复制" data-report-click="{&quot;spm&quot;:&quot;1001.2101.3001.4334&quot;}" onclick="hljs.signin(event)"></div></pre> 
<p><strong>初次加载模型</strong></p> 
<pre class="set-code-hide" name="code"><code class="language-python hljs"><ol class="hljs-ln" style="width:100%"><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="1"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-comment"># 加载模型</span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="2"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">model = SVC()</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="3"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"> </div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="4"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-comment"># 训练模型</span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="5"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">model.fit(X_train,y_train)</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="6"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-comment"># 预测值</span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="7"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">y_pred = model.predict(X_test)</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="8"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"> </div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="9"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-string"><span class="hljs-string">'''</span></span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="10"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-string">评估指标</span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="11"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-string">'''</span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="12"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-comment"># 求出预测和真实一样的数目</span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="13"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">true = np.<span class="hljs-built_in">sum</span>(y_pred == y_test )</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="14"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-built_in">print</span>(<span class="hljs-string">'预测对的结果数目为：'</span>, true)</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="15"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-built_in">print</span>(<span class="hljs-string">'预测错的的结果数目为：'</span>, y_test.shape[<span class="hljs-number">0</span>]-true)</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="16"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-comment"># 评估指标</span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="17"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> accuracy_score,precision_score,recall_score,f1_score,cohen_kappa_score</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="18"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-built_in">print</span>(<span class="hljs-string">'预测数据的准确率为： {:.4}%'</span>.<span class="hljs-built_in">format</span>(accuracy_score(y_test,y_pred)*<span class="hljs-number">100</span>))</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="19"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-built_in">print</span>(<span class="hljs-string">'预测数据的精确率为：{:.4}%'</span>.<span class="hljs-built_in">format</span>(</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="20"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">      precision_score(y_test,y_pred)*<span class="hljs-number">100</span>))</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="21"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-built_in">print</span>(<span class="hljs-string">'预测数据的召回率为：{:.4}%'</span>.<span class="hljs-built_in">format</span>(</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="22"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">      recall_score(y_test,y_pred)*<span class="hljs-number">100</span>))</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="23"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-comment"># print("训练数据的F1值为：", f1score_train)</span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="24"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-built_in">print</span>(<span class="hljs-string">'预测数据的F1值为：'</span>,</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="25"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">      f1_score(y_test,y_pred))</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="26"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-built_in">print</span>(<span class="hljs-string">'预测数据的Cohen’s Kappa系数为：'</span>,</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="27"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">      cohen_kappa_score(y_test,y_pred))</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="28"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-comment"># 打印分类报告</span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="29"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-built_in">print</span>(<span class="hljs-string">'预测数据的分类报告为：'</span>,<span class="hljs-string">'\n'</span>,</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="30"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">      classification_report(y_test,y_pred))</div></div></li></ol></code><div class="hide-preCode-box"><span class="hide-preCode-bt" data-report-view="{&quot;spm&quot;:&quot;1001.2101.3001.7365&quot;}"><img class="look-more-preCode contentImg-no-view" src="https://csdnimg.cn/release/blogv2/dist/pc/img/newCodeMoreWhite.png" alt="" title=""></span></div><div class="hljs-button signin" data-title="登录后复制" data-report-click="{&quot;spm&quot;:&quot;1001.2101.3001.4334&quot;}" onclick="hljs.signin(event)"></div></pre> 
<p><img alt="" height="362" src="https://img-blog.csdnimg.cn/427216c2629345279fd745d6645a6c08.png" width="983"></p> 
<p><strong>这里默认使用的是高斯核，但是我们的数据集范围并没有规约到[0,1]之间，由于数据集最先最好了离散化，所以造成的影响也不是很大，因为这里是初次加载</strong></p> 
<p><strong>标准化</strong></p> 
<pre><code class="language-python hljs"><ol class="hljs-ln" style="width:100%"><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="1"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-comment"># 没有作用</span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="2"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-comment"># sc = StandardScaler()</span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="3"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-comment"># 标准化【0,1】</span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="4"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-comment"># 效果不行</span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="5"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">sc=MinMaxScaler() </div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="6"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-comment"># sc=MaxAbsScaler()</span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="7"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">X_train1 = sc.fit_transform(X_train)</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="8"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">X_test1 = sc.transform(X_test)</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="9"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">X_test1</div></div></li></ol></code><div class="hljs-button signin" data-title="登录后复制" data-report-click="{&quot;spm&quot;:&quot;1001.2101.3001.4334&quot;}" onclick="hljs.signin(event)"></div></pre> 
<p><img alt="" height="167" src="https://img-blog.csdnimg.cn/2d369203f0d04136b48f921226567c7b.png" width="701"></p> 
<p><strong>使用标准化后的数据集进行预测</strong></p> 
<pre class="set-code-hide" name="code"><code class="language-python hljs"><ol class="hljs-ln" style="width:100%"><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="1"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-comment"># 加载模型</span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="2"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">model = SVC()</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="3"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"> </div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="4"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-comment"># 训练模型</span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="5"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">model.fit(X_train1,y_train)</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="6"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-comment"># 预测值</span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="7"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">y_pred = model.predict(X_test1)</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="8"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"> </div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="9"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-string"><span class="hljs-string">'''</span></span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="10"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-string">评估指标</span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="11"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-string">'''</span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="12"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-comment"># 求出预测和真实一样的数目</span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="13"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">true = np.<span class="hljs-built_in">sum</span>(y_pred == y_test )</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="14"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-built_in">print</span>(<span class="hljs-string">'预测对的结果数目为：'</span>, true)</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="15"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-built_in">print</span>(<span class="hljs-string">'预测错的的结果数目为：'</span>, y_test.shape[<span class="hljs-number">0</span>]-true)</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="16"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-comment"># 评估指标</span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="17"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> accuracy_score,precision_score,recall_score,f1_score,cohen_kappa_score</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="18"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-built_in">print</span>(<span class="hljs-string">'预测数据的准确率为： {:.4}%'</span>.<span class="hljs-built_in">format</span>(accuracy_score(y_test,y_pred)*<span class="hljs-number">100</span>))</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="19"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-built_in">print</span>(<span class="hljs-string">'预测数据的精确率为：{:.4}%'</span>.<span class="hljs-built_in">format</span>(</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="20"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">      precision_score(y_test,y_pred)*<span class="hljs-number">100</span>))</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="21"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-built_in">print</span>(<span class="hljs-string">'预测数据的召回率为：{:.4}%'</span>.<span class="hljs-built_in">format</span>(</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="22"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">      recall_score(y_test,y_pred)*<span class="hljs-number">100</span>))</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="23"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-comment"># print("训练数据的F1值为：", f1score_train)</span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="24"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-built_in">print</span>(<span class="hljs-string">'预测数据的F1值为：'</span>,</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="25"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">      f1_score(y_test,y_pred))</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="26"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-built_in">print</span>(<span class="hljs-string">'预测数据的Cohen’s Kappa系数为：'</span>,</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="27"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">      cohen_kappa_score(y_test,y_pred))</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="28"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-comment"># 打印分类报告</span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="29"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-built_in">print</span>(<span class="hljs-string">'预测数据的分类报告为：'</span>,<span class="hljs-string">'\n'</span>,</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="30"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">      classification_report(y_test,y_pred))</div></div></li></ol></code><div class="hide-preCode-box"><span class="hide-preCode-bt" data-report-view="{&quot;spm&quot;:&quot;1001.2101.3001.7365&quot;}"><img class="look-more-preCode contentImg-no-view" src="https://csdnimg.cn/release/blogv2/dist/pc/img/newCodeMoreWhite.png" alt="" title=""></span></div><div class="hljs-button signin" data-title="登录后复制" data-report-click="{&quot;spm&quot;:&quot;1001.2101.3001.4334&quot;}" onclick="hljs.signin(event)"></div></pre> 
<p><img alt="" height="375" src="https://img-blog.csdnimg.cn/a48ef1378df940cbbbfe06cf4b9c6565.png" width="1127"></p> 
<p>&nbsp;<strong>并不乐观，继续探索</strong></p> 
<h3 id="%E6%A8%A1%E5%9E%8B%E8%B0%83%E5%8F%82"><a name="t12"></a><strong>模型调参</strong></h3> 
<pre><code class="language-python hljs"><ol class="hljs-ln" style="width:100%"><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="1"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-keyword">from</span> datetime <span class="hljs-keyword">import</span> time</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="2"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-keyword">import</span> datetime</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="3"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-comment"># ploy在该例中跑不出来</span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="4"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">Kernel = [<span class="hljs-string">"linear"</span>,  <span class="hljs-string">"rbf"</span>, <span class="hljs-string">"sigmoid"</span>,<span class="hljs-string">"poly"</span>]</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="5"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-keyword">for</span> kernel <span class="hljs-keyword">in</span> Kernel:</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="6"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">    time0 = time()</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="7"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">    clf = SVC(kernel=kernel,</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="8"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">              gamma=<span class="hljs-string">"auto"</span>,</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="9"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">              cache_size=<span class="hljs-number">5000</span>,   <span class="hljs-comment"># 允许使用的内存，单位为MB，默认是200M</span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="10"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">              ).fit(X_train, y_train)</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="11"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">    <span class="hljs-built_in">print</span>(<span class="hljs-string">"The accuracy under kernel %s is %f"</span> % (kernel, clf.score(X_test, y_test)))</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="12"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-comment">#     print(datetime.datetime.fromtimestamp(time() - time0).strftime("%M:%S:%f"))</span></div></div></li></ol></code><div class="hljs-button signin" data-title="登录后复制" data-report-click="{&quot;spm&quot;:&quot;1001.2101.3001.4334&quot;}" onclick="hljs.signin(event)"></div></pre> 
<p><img alt="" height="104" src="https://img-blog.csdnimg.cn/6e82b82f018c49e8b45e53fcf7571a82.png" width="888"></p> 
<p><strong>优先考虑多项式和高斯核</strong></p> 
<h3 id="gamma%E8%B0%83%E5%8F%82"><a name="t13"></a><strong>gamma调参</strong></h3> 
<p><strong>RBF调参</strong></p> 
<pre class="set-code-hide" name="code"><code class="language-python hljs"><ol class="hljs-ln" style="width:100%"><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="1"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-comment"># 使用rbf，必须要对其进行数据缩放</span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="2"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-comment"># 画学习曲线</span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="3"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">score = []</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="4"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">gamma_range = np.logspace(-<span class="hljs-number">10</span>, <span class="hljs-number">1</span>, <span class="hljs-number">50</span>)  <span class="hljs-comment"># 返回在对数刻度上均匀间隔的数字</span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="5"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> gamma_range:</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="6"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">    clf = SVC(kernel=<span class="hljs-string">"rbf"</span>, gamma=i, cache_size=<span class="hljs-number">5000</span>).fit(X_train, y_train)</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="7"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">    score.append(clf.score(X_test, y_test))</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="8"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-built_in">print</span>(<span class="hljs-built_in">max</span>(score), gamma_range[score.index(<span class="hljs-built_in">max</span>(score))])</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="9"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">best_gamma_rbf=gamma_range[score.index(<span class="hljs-built_in">max</span>(score))]</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="10"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-comment">#设置标题</span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="11"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">plt. title(<span class="hljs-string">f' SVC (rbf) <span class="hljs-subst">{<span class="hljs-built_in">max</span>(score)}</span>'</span>)</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="12"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-comment">#设置x轴标签</span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="13"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">plt. xlabel(<span class="hljs-string">' gamma'</span>)</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="14"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-comment">#设置y轴标签</span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="15"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">plt. ylabel(<span class="hljs-string">' Score'</span>)</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="16"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-comment">#添加图例</span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="17"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-comment"># plt. legend()</span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="18"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">plt.plot(gamma_range, score)</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="19"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">plt.show()</div></div></li></ol></code><div class="hide-preCode-box"><span class="hide-preCode-bt" data-report-view="{&quot;spm&quot;:&quot;1001.2101.3001.7365&quot;}"><img class="look-more-preCode contentImg-no-view" src="https://csdnimg.cn/release/blogv2/dist/pc/img/newCodeMoreWhite.png" alt="" title=""></span></div><div class="hljs-button signin" data-title="登录后复制" data-report-click="{&quot;spm&quot;:&quot;1001.2101.3001.4334&quot;}" onclick="hljs.signin(event)"></div></pre> 
<blockquote> 
 <p><img alt="" height="397" src="https://img-blog.csdnimg.cn/b1703acd29384668bf5ab35297b414e4.png" width="866"></p> 
 <p>rbf核下的最佳gamma是0.1599......</p> 
</blockquote> 
<p><strong>poly调参</strong></p> 
<pre class="set-code-hide" name="code"><code class="language-python hljs"><ol class="hljs-ln" style="width:100%"><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="1"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-comment"># 画学习曲线,数据不需要处理</span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="2"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">score = []</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="3"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">gamma_range = np.logspace(-<span class="hljs-number">10</span>, <span class="hljs-number">1</span>, <span class="hljs-number">50</span>)  <span class="hljs-comment"># 返回在对数刻度上均匀间隔的数字</span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="4"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> gamma_range:</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="5"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">    clf = SVC(kernel=<span class="hljs-string">"poly"</span>, gamma=i, cache_size=<span class="hljs-number">5000</span>).fit(X_train, y_train)</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="6"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">    score.append(clf.score(X_test, y_test))</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="7"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-built_in">print</span>(<span class="hljs-built_in">max</span>(score), gamma_range[score.index(<span class="hljs-built_in">max</span>(score))])</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="8"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">best_gamma_poly=gamma_range[score.index(<span class="hljs-built_in">max</span>(score))]</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="9"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-comment">#设置标题</span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="10"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">plt. title(<span class="hljs-string">f' SVC (poly) <span class="hljs-subst">{<span class="hljs-built_in">max</span>(score)}</span>'</span>)</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="11"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-comment">#设置x轴标签</span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="12"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">plt. xlabel(<span class="hljs-string">' gamma'</span>)</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="13"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-comment">#设置y轴标签</span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="14"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">plt. ylabel(<span class="hljs-string">' Score'</span>)</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="15"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-comment">#添加图例</span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="16"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-comment"># plt. legend()</span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="17"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">plt.plot(gamma_range, score)</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="18"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">plt.show()</div></div></li></ol></code><div class="hide-preCode-box"><span class="hide-preCode-bt" data-report-view="{&quot;spm&quot;:&quot;1001.2101.3001.7365&quot;}"><img class="look-more-preCode contentImg-no-view" src="https://csdnimg.cn/release/blogv2/dist/pc/img/newCodeMoreWhite.png" alt="" title=""></span></div><div class="hljs-button signin" data-title="登录后复制" data-report-click="{&quot;spm&quot;:&quot;1001.2101.3001.4334&quot;}" onclick="hljs.signin(event)"></div></pre> 
<blockquote> 
 <p><img alt="" height="408" src="https://img-blog.csdnimg.cn/a2818b34d77040949a67fabebdbfd908.png" width="984"></p> 
 <p>效果上升了，感觉多项式核函数也还不错的</p> 
</blockquote> 
<h3 id="C%E5%80%BC%E8%B0%83%E5%8F%82"><a name="t14"></a>C值调参</h3> 
<pre class="set-code-hide" name="code"><code class="language-python hljs"><ol class="hljs-ln" style="width:100%"><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="1"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-comment"># 调线性核函数</span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="2"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">score = []</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="3"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">C_range = np.linspace(<span class="hljs-number">0.01</span>, <span class="hljs-number">30</span>, <span class="hljs-number">50</span>)</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="4"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> C_range:</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="5"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">    clf = SVC(kernel=<span class="hljs-string">"linear"</span>,C=i,cache_size=<span class="hljs-number">5000</span>).fit(X_train,y_train)</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="6"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">    score.append(clf.score(X_test, y_test))</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="7"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-built_in">print</span>(<span class="hljs-built_in">max</span>(score), C_range[score.index(<span class="hljs-built_in">max</span>(score))])</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="8"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">best_C_linear=C_range[score.index(<span class="hljs-built_in">max</span>(score))]</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="9"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-comment">#设置标题</span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="10"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">plt. title(<span class="hljs-string">f' SVC (linspace) <span class="hljs-subst">{<span class="hljs-built_in">max</span>(score)}</span>'</span>)</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="11"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-comment">#设置x轴标签</span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="12"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">plt. xlabel(<span class="hljs-string">' C'</span>)</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="13"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-comment">#设置y轴标签</span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="14"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">plt. ylabel(<span class="hljs-string">' Score'</span>)</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="15"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-comment">#添加图例</span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="16"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-comment"># plt. legend()</span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="17"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">plt.plot(C_range, score)</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="18"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">plt.show()</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="19"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"> </div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="20"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-comment"># 换rbf,并且这里对数据进行了标准化，缩放到0和1之间的</span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="21"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">score = []</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="22"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">C_range = np.linspace(<span class="hljs-number">0.01</span>, <span class="hljs-number">30</span>, <span class="hljs-number">50</span>)</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="23"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> C_range:</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="24"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">    clf = SVC(kernel=<span class="hljs-string">"rbf"</span>, C=i, gamma=<span class="hljs-number">0.15998587196060574</span>, cache_size=<span class="hljs-number">5000</span>).fit(X_train, y_train)</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="25"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">    score.append(clf.score(X_test, y_test))</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="26"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-built_in">print</span>(<span class="hljs-built_in">max</span>(score), C_range[score.index(<span class="hljs-built_in">max</span>(score))])</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="27"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">best_C_rbf=C_range[score.index(<span class="hljs-built_in">max</span>(score))]</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="28"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-comment">#设置标题</span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="29"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">plt. title(<span class="hljs-string">f' SVC (rbf) <span class="hljs-subst">{<span class="hljs-built_in">max</span>(score)}</span>'</span>)</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="30"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-comment">#设置x轴标签</span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="31"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">plt. xlabel(<span class="hljs-string">' C'</span>)</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="32"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-comment">#设置y轴标签</span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="33"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">plt. ylabel(<span class="hljs-string">' Score'</span>)</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="34"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-comment">#添加图例</span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="35"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-comment"># plt. legend()</span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="36"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">plt.plot(C_range, score)</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="37"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">plt.show()</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="38"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"> </div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="39"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"> </div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="40"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-comment"># 换ploy</span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="41"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">score = []</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="42"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">C_range = np.linspace(<span class="hljs-number">0.01</span>, <span class="hljs-number">30</span>, <span class="hljs-number">50</span>)</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="43"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> C_range:</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="44"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">    clf = SVC(kernel=<span class="hljs-string">"poly"</span>, C=i, gamma=<span class="hljs-number">0.0339322177189533</span>, cache_size=<span class="hljs-number">5000</span>).fit(X_train, y_train)</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="45"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">    score.append(clf.score(X_test, y_test))</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="46"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">    </div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="47"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-built_in">print</span>(<span class="hljs-built_in">max</span>(score), C_range[score.index(<span class="hljs-built_in">max</span>(score))])</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="48"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">best_C_poly=C_range[score.index(<span class="hljs-built_in">max</span>(score))]</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="49"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-comment">#设置标题</span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="50"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">plt. title(<span class="hljs-string">f' SVC (poly) <span class="hljs-subst">{<span class="hljs-built_in">max</span>(score)}</span>'</span>)</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="51"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-comment">#设置x轴标签</span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="52"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">plt. xlabel(<span class="hljs-string">' C'</span>)</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="53"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-comment">#设置y轴标签</span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="54"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">plt. ylabel(<span class="hljs-string">' Score'</span>)</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="55"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-comment">#添加图例</span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="56"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-comment"># plt. legend()</span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="57"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">plt.plot(C_range, score)</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="58"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">plt.show()</div></div></li></ol></code><div class="hide-preCode-box"><span class="hide-preCode-bt" data-report-view="{&quot;spm&quot;:&quot;1001.2101.3001.7365&quot;}"><img class="look-more-preCode contentImg-no-view" src="https://csdnimg.cn/release/blogv2/dist/pc/img/newCodeMoreWhite.png" alt="" title=""></span></div><div class="hljs-button signin" data-title="登录后复制" data-report-click="{&quot;spm&quot;:&quot;1001.2101.3001.4334&quot;}" onclick="hljs.signin(event)"></div></pre> 
<blockquote> 
 <p><img alt="" height="1200" src="https://img-blog.csdnimg.cn/4eb7e555336d4c28a9eea69ddadde797.png" width="896"></p> 
 <p><strong>看起来他们的分数不相上下（RBF和poly），根据他们的学习曲线，还是可以看出不一样的</strong></p> 
</blockquote> 
<h3 id="%E4%BD%BF%E7%94%A8Polynomial%20kernel%E8%BF%9B%E8%A1%8C%E9%A2%84%E6%B5%8B"><a name="t15"></a>使用Polynomial kernel进行预测</h3> 
<pre class="set-code-hide" name="code"><code class="language-python hljs"><ol class="hljs-ln" style="width:968px"><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="1"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-comment"># 加载模型</span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="2"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">model_1 = SVC(C=best_C_poly,gamma=best_gamma_poly,kernel=<span class="hljs-string">'poly'</span>,cache_size=<span class="hljs-number">5000</span>,degree=<span class="hljs-number">3</span>,probability=<span class="hljs-literal">True</span>)</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="3"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"> </div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="4"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-comment"># 训练模型</span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="5"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">model_1.fit(X_train,y_train)</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="6"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-comment"># 预测值</span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="7"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">y_pred = model_1.predict(X_test)</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="8"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"> </div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="9"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-string"><span class="hljs-string">'''</span></span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="10"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-string">评估指标</span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="11"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-string">'''</span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="12"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-comment"># 求出预测和真实一样的数目</span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="13"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">true = np.<span class="hljs-built_in">sum</span>(y_pred == y_test )</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="14"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-built_in">print</span>(<span class="hljs-string">'预测对的结果数目为：'</span>, true)</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="15"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-built_in">print</span>(<span class="hljs-string">'预测错的的结果数目为：'</span>, y_test.shape[<span class="hljs-number">0</span>]-true)</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="16"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-comment"># 评估指标</span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="17"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> accuracy_score,precision_score,recall_score,f1_score,cohen_kappa_score</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="18"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-built_in">print</span>(<span class="hljs-string">'预测数据的准确率为： {:.4}%'</span>.<span class="hljs-built_in">format</span>(accuracy_score(y_test,y_pred)*<span class="hljs-number">100</span>))</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="19"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-built_in">print</span>(<span class="hljs-string">'预测数据的精确率为：{:.4}%'</span>.<span class="hljs-built_in">format</span>(</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="20"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">      precision_score(y_test,y_pred)*<span class="hljs-number">100</span>))</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="21"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-built_in">print</span>(<span class="hljs-string">'预测数据的召回率为：{:.4}%'</span>.<span class="hljs-built_in">format</span>(</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="22"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">      recall_score(y_test,y_pred)*<span class="hljs-number">100</span>))</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="23"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-comment"># print("训练数据的F1值为：", f1score_train)</span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="24"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-built_in">print</span>(<span class="hljs-string">'预测数据的F1值为：'</span>,</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="25"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">      f1_score(y_test,y_pred))</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="26"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-built_in">print</span>(<span class="hljs-string">'预测数据的Cohen’s Kappa系数为：'</span>,</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="27"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">      cohen_kappa_score(y_test,y_pred))</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="28"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-comment"># 打印分类报告</span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="29"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-built_in">print</span>(<span class="hljs-string">'预测数据的分类报告为：'</span>,<span class="hljs-string">'\n'</span>,</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="30"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">      classification_report(y_test,y_pred))</div></div></li></ol></code><div class="hide-preCode-box"><span class="hide-preCode-bt" data-report-view="{&quot;spm&quot;:&quot;1001.2101.3001.7365&quot;}"><img class="look-more-preCode contentImg-no-view" src="https://csdnimg.cn/release/blogv2/dist/pc/img/newCodeMoreWhite.png" alt="" title=""></span></div><div class="hljs-button signin" data-title="登录后复制" data-report-click="{&quot;spm&quot;:&quot;1001.2101.3001.4334&quot;}" onclick="hljs.signin(event)"></div></pre> 
<pre class="set-code-hide" name="code"><code class="language-python hljs"><ol class="hljs-ln" style="width:100%"><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="1"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> precision_recall_curve</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="2"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> metrics</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="3"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-comment"># 预测正例的概率</span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="4"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">y_pred_prob=model_1.predict_proba(X_test)[:,<span class="hljs-number">1</span>]</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="5"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-comment"># y_pred_prob ,返回两列，第一列代表类别0,第二列代表类别1的概率</span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="6"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-comment">#https://blog.csdn.net/dream6104/article/details/89218239</span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="7"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">fpr, tpr, thresholds = metrics.roc_curve(y_test,y_pred_prob, pos_label=<span class="hljs-number">2</span>)</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="8"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-comment">#pos_label，代表真阳性标签，就是说是分类里面的好的标签，这个要看你的特征目标标签是0,1，还是1,2</span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="9"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">roc_auc = metrics.auc(fpr, tpr)  <span class="hljs-comment">#auc为Roc曲线下的面积</span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="10"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-comment"># print(roc_auc)</span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="11"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">plt.figure(figsize=(<span class="hljs-number">8</span>,<span class="hljs-number">6</span>))</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="12"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">plt.plot([<span class="hljs-number">0</span>, <span class="hljs-number">1</span>], [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>], color=<span class="hljs-string">'navy'</span>, lw=<span class="hljs-number">2</span>, linestyle=<span class="hljs-string">'--'</span>)</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="13"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">plt.plot(fpr, tpr, <span class="hljs-string">'r'</span>,label=<span class="hljs-string">'AUC = %0.2f'</span>% roc_auc)</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="14"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">plt.legend(loc=<span class="hljs-string">'lower right'</span>)</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="15"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-comment"># plt.plot([0, 1], [0, 1], 'r--')</span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="16"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">plt.xlim([<span class="hljs-number">0</span>, <span class="hljs-number">1.1</span>])</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="17"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">plt.ylim([<span class="hljs-number">0</span>, <span class="hljs-number">1.1</span>])</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="18"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">plt.xlabel(<span class="hljs-string">'False Positive Rate'</span>) <span class="hljs-comment">#横坐标是fpr</span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="19"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">plt.ylabel(<span class="hljs-string">'True Positive Rate'</span>)  <span class="hljs-comment">#纵坐标是tpr</span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="20"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">plt.title(<span class="hljs-string">'Receiver operating characteristic example'</span>)</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="21"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">plt.show()</div></div></li></ol></code><div class="hide-preCode-box"><span class="hide-preCode-bt" data-report-view="{&quot;spm&quot;:&quot;1001.2101.3001.7365&quot;}"><img class="look-more-preCode contentImg-no-view" src="https://csdnimg.cn/release/blogv2/dist/pc/img/newCodeMoreWhite.png" alt="" title=""></span></div><div class="hljs-button signin" data-title="登录后复制" data-report-click="{&quot;spm&quot;:&quot;1001.2101.3001.4334&quot;}" onclick="hljs.signin(event)"></div></pre> 
<p>&nbsp;<strong>评估指标</strong></p> 
<blockquote> 
 <p><img alt="" height="373" src="https://img-blog.csdnimg.cn/546be11bd2454251bcc83bbd24da6335.png" width="1025"></p> 
 <p><strong>&nbsp;ROC曲线AUC面积</strong><img alt="" height="487" src="https://img-blog.csdnimg.cn/f2fa03418c1f48dfa8ed814d83d23cb8.png" width="974"></p> 
 <p>&nbsp;模型效果还是不错，可以达到93%的准确率</p> 
</blockquote> 
<h3 id="%E4%BD%BF%E7%94%A8RBF%20kernel%E8%BF%9B%E8%A1%8C%E9%A2%84%E6%B5%8B"><a name="t16"></a>使用RBF kernel进行预测</h3> 
<pre class="set-code-hide" name="code"><code class="language-python hljs"><ol class="hljs-ln" style="width:100%"><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="1"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-comment"># 加载模型</span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="2"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">model_2 = SVC(C=best_C_rbf,kernel=<span class="hljs-string">'rbf'</span>,cache_size=<span class="hljs-number">5000</span>,probability=<span class="hljs-literal">True</span>)</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="3"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"> </div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="4"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-comment"># 训练模型</span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="5"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">model_2.fit(X_train1,y_train)</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="6"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-comment"># 预测值</span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="7"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">y_pred = model_2.predict(X_test1)</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="8"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"> </div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="9"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-string"><span class="hljs-string">'''</span></span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="10"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-string">评估指标</span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="11"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-string">'''</span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="12"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-comment"># 求出预测和真实一样的数目</span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="13"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">true = np.<span class="hljs-built_in">sum</span>(y_pred == y_test )</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="14"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-built_in">print</span>(<span class="hljs-string">'预测对的结果数目为：'</span>, true)</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="15"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-built_in">print</span>(<span class="hljs-string">'预测错的的结果数目为：'</span>, y_test.shape[<span class="hljs-number">0</span>]-true)</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="16"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-comment"># 评估指标</span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="17"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> accuracy_score,precision_score,recall_score,f1_score,cohen_kappa_score</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="18"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-built_in">print</span>(<span class="hljs-string">'预测数据的准确率为： {:.4}%'</span>.<span class="hljs-built_in">format</span>(accuracy_score(y_test,y_pred)*<span class="hljs-number">100</span>))</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="19"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-built_in">print</span>(<span class="hljs-string">'预测数据的精确率为：{:.4}%'</span>.<span class="hljs-built_in">format</span>(</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="20"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">      precision_score(y_test,y_pred)*<span class="hljs-number">100</span>))</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="21"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-built_in">print</span>(<span class="hljs-string">'预测数据的召回率为：{:.4}%'</span>.<span class="hljs-built_in">format</span>(</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="22"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">      recall_score(y_test,y_pred)*<span class="hljs-number">100</span>))</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="23"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-comment"># print("训练数据的F1值为：", f1score_train)</span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="24"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-built_in">print</span>(<span class="hljs-string">'预测数据的F1值为：'</span>,</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="25"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">      f1_score(y_test,y_pred))</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="26"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-built_in">print</span>(<span class="hljs-string">'预测数据的Cohen’s Kappa系数为：'</span>,</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="27"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">      cohen_kappa_score(y_test,y_pred))</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="28"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-comment"># 打印分类报告</span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="29"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-built_in">print</span>(<span class="hljs-string">'预测数据的分类报告为：'</span>,<span class="hljs-string">'\n'</span>,</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="30"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">      classification_report(y_test,y_pred))</div></div></li></ol></code><div class="hide-preCode-box"><span class="hide-preCode-bt" data-report-view="{&quot;spm&quot;:&quot;1001.2101.3001.7365&quot;}"><img class="look-more-preCode contentImg-no-view" src="https://csdnimg.cn/release/blogv2/dist/pc/img/newCodeMoreWhite.png" alt="" title=""></span></div><div class="hljs-button signin" data-title="登录后复制" data-report-click="{&quot;spm&quot;:&quot;1001.2101.3001.4334&quot;}" onclick="hljs.signin(event)"></div></pre> 
<pre class="set-code-hide" name="code"><code class="language-python hljs"><ol class="hljs-ln" style="width:100%"><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="1"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> precision_recall_curve</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="2"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> metrics</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="3"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-comment"># 预测正例的概率</span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="4"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">y_pred_prob=model_2.predict_proba(X_test1)[:,<span class="hljs-number">1</span>]</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="5"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-comment"># y_pred_prob ,返回两列，第一列代表类别0,第二列代表类别1的概率</span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="6"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-comment">#https://blog.csdn.net/dream6104/article/details/89218239</span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="7"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">fpr, tpr, thresholds = metrics.roc_curve(y_test,y_pred_prob, pos_label=<span class="hljs-number">2</span>)</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="8"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-comment">#pos_label，代表真阳性标签，就是说是分类里面的好的标签，这个要看你的特征目标标签是0,1，还是1,2</span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="9"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">roc_auc = metrics.auc(fpr, tpr)  <span class="hljs-comment">#auc为Roc曲线下的面积</span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="10"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-comment"># print(roc_auc)</span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="11"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">plt.figure(figsize=(<span class="hljs-number">8</span>,<span class="hljs-number">6</span>))</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="12"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">plt.plot([<span class="hljs-number">0</span>, <span class="hljs-number">1</span>], [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>], color=<span class="hljs-string">'navy'</span>, lw=<span class="hljs-number">2</span>, linestyle=<span class="hljs-string">'--'</span>)</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="13"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">plt.plot(fpr, tpr, <span class="hljs-string">'r'</span>,label=<span class="hljs-string">'AUC = %0.2f'</span>% roc_auc)</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="14"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">plt.legend(loc=<span class="hljs-string">'lower right'</span>)</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="15"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line"><span class="hljs-comment"># plt.plot([0, 1], [0, 1], 'r--')</span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="16"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">plt.xlim([<span class="hljs-number">0</span>, <span class="hljs-number">1.1</span>])</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="17"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">plt.ylim([<span class="hljs-number">0</span>, <span class="hljs-number">1.1</span>])</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="18"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">plt.xlabel(<span class="hljs-string">'False Positive Rate'</span>) <span class="hljs-comment">#横坐标是fpr</span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="19"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">plt.ylabel(<span class="hljs-string">'True Positive Rate'</span>)  <span class="hljs-comment">#纵坐标是tpr</span></div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="20"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">plt.title(<span class="hljs-string">'Receiver operating characteristic example'</span>)</div></div></li><li><div class="hljs-ln-numbers"><div class="hljs-ln-line hljs-ln-n" data-line-number="21"></div></div><div class="hljs-ln-code"><div class="hljs-ln-line">plt.show()</div></div></li></ol></code><div class="hide-preCode-box"><span class="hide-preCode-bt" data-report-view="{&quot;spm&quot;:&quot;1001.2101.3001.7365&quot;}"><img class="look-more-preCode contentImg-no-view" src="https://csdnimg.cn/release/blogv2/dist/pc/img/newCodeMoreWhite.png" alt="" title=""></span></div><div class="hljs-button signin" data-title="登录后复制" data-report-click="{&quot;spm&quot;:&quot;1001.2101.3001.4334&quot;}" onclick="hljs.signin(event)"></div></pre> 
<blockquote> 
 <p><img alt="" height="378" src="https://img-blog.csdnimg.cn/8daeda217d7349d485079eccc9007343.png" width="776"></p> 
 <p><img alt="" height="490" src="https://img-blog.csdnimg.cn/f7aaf9bb5f564cf185511aafa1d281de.png" width="707"><strong>效果也还是不错，虽然比poly稍微低一点，但是总体来说还是不错的</strong></p> 
</blockquote> 
<h3 id="%E6%80%BB%E7%BB%93"><a name="t17"></a>总结</h3> 
<p>本次使用支持向量机进行模型分类预测，并没有对其进行特征筛选，效果也是不错的，因为支持向量机的本质也会根据根据特征进行划分，这里经过测试之后也确实如此。</p> 
<p>核支持向量机是非常强大的模型，在各种数据集上的表现都很好。 SVM 允许决策边界很<br> 复杂，即使数据只有几个特征。它在低维数据和高维数据（即很少特征和很多特征）上的<br> 表现都很好，但对样本个数的缩放表现不好。在有多达 10 000 个样本的数据上运行 SVM<br> 可能表现良好，但如果数据量达到 100 000 甚至更大，在运行时间和内存使用方面可能会<br> 面临挑战。</p> 
<p>SVM 的另一个缺点是，<strong>预处理数据和调参都需要非常小心。这也是为什么如今很多应用<br> 中用的都是基于树的模型，比如随机森林或梯度提升（需要很少的预处理，甚至不需要预<br> 处理）。此外， SVM 模型很难检查，可能很难理解为什么会这么预测，而且也难以将模型<br> 向非专家进行解释。</strong><br> 不过 SVM 仍然是值得尝试的，特别是所有特征的测量单位相似（比如都是像素密度）而<br> 且范围也差不多时。<br><strong>核 SVM 的重要参数是正则化参数 C、核的选择以及与核相关的参数。</strong>虽然我们主要讲的是<br> RBF 核，但 scikit-learn 中还有其他选择。 RBF 核只有一个参数 gamma，它是高斯核宽度<br> 的倒数。</p> 
<p>gamma 和 C 控制的都是模型复杂度，较大的值都对应更为复杂的模型。因此，这<br> 两个参数的设定通常是强烈相关的，<strong><span style="color:#fe2c24;">应该同时调节。</span></strong></p> 
<h3 id="%E6%AF%8F%E6%96%87%E4%B8%80%E8%AF%AD"><a name="t18"></a>每文一语</h3> 
<blockquote> 
 <p><strong>走好当下的每一步便是努力</strong></p> 
</blockquote>
                </div>